name: Deploy Frontend to EKS

on:
  workflow_dispatch: {}
  workflow_call: {}         # <—
  push:
    paths:
      - ".github/workflows/deploy_frontend.yml"
      - "frontend/**"
      - "infrastructure/k8s/frontend/**"
      - "infrastructure/k8s/ingress/frontend-ingress.yaml"
      - "infrastructure/k8s/namespaces/nat20.yaml"

permissions:
  id-token: write
  contents: read

concurrency:
  group: nat20-eks-ops       # <—
  cancel-in-progress: false

env:
  AWS_REGION: eu-central-1
  CLUSTER_NAME: nat20-eks
  PROJECT_NAME: nat20
  NAMESPACE: nat20
  ECR_REPO: nat20/frontend
  KUBECTL_TIMEOUT: 30s
  AWS_RETRY_MODE: standard
  AWS_MAX_ATTEMPTS: 6
  # Optional override. Leave blank to auto-detect from CloudFront distribution comment.
  DOMAIN_NAME: ""

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install tooling
        shell: bash
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y jq curl ca-certificates gettext-base
          sudo update-ca-certificates

      - name: ECR login
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Compute image URI
        id: img
        shell: bash
        run: |
          set -euo pipefail
          echo "ECR_REGISTRY=${{ steps.login-ecr.outputs.registry }}" >> "$GITHUB_ENV"
          echo "IMAGE_URI=${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPO }}:sha-${{ github.sha }}" >> "$GITHUB_ENV"
          echo "uri=${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPO }}:sha-${{ github.sha }}" >> "$GITHUB_OUTPUT"

      - name: Build & push image
        shell: bash
        env:
          DOCKER_BUILDKIT: "1"
        run: |
          set -euo pipefail
          docker build -f infrastructure/docker/frontend/Dockerfile -t "${IMAGE_URI}" .
          docker push "${IMAGE_URI}"

      - name: Read SSM params (ingress cert + SG)
        shell: bash
        run: |
          set -euo pipefail
          ORIGIN_CERT_ARN="$(aws ssm get-parameter --name /${PROJECT_NAME}/network/ORIGIN_CERT_ARN --query 'Parameter.Value' --output text)"
          ALB_FRONTEND_SG_ID="$(aws ssm get-parameter --name /${PROJECT_NAME}/network/ALB_FRONTEND_SG_ID --query 'Parameter.Value' --output text)"
          echo "ORIGIN_CERT_ARN=${ORIGIN_CERT_ARN}" >> "$GITHUB_ENV"
          echo "ALB_FRONTEND_SG_ID=${ALB_FRONTEND_SG_ID}" >> "$GITHUB_ENV"

      - name: kubeconfig
        shell: bash
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}"
          kubectl version --client || true

      - name: "Preflight API reachability (200/401/403 OK)"
        id: preflight
        shell: bash
        run: |
          set -euo pipefail
          ENDPOINT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query 'cluster.endpoint' --output text)"
          ok="false"
          for i in 1 2 3; do
            code="$(curl -k -sS -o /dev/null -w '%{http_code}' --connect-timeout 5 --max-time 10 "${ENDPOINT}/version" || echo 000)"
            if [[ "$code" == "200" || "$code" == "401" || "$code" == "403" ]]; then
              ok="true"; break
            fi
            sleep 5
          done
          echo "ok=${ok}" >> "$GITHUB_OUTPUT"

      - name: "Fallback allow runner CIDR"
        if: steps.preflight.outputs.ok != 'true'
        shell: bash
        run: |
          set -euo pipefail
          RUNNER_CIDR="$(curl -s https://checkip.amazonaws.com | tr -d '\r\n')/32"
          echo "RUNNER_CIDR=${RUNNER_CIDR}" >> "$GITHUB_ENV"

          CUR_JSON="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output json)"
          if [[ -z "${CUR_JSON}" || "${CUR_JSON}" == "null" ]]; then CUR_JSON='[]'; fi
          NEW_CSV="$(printf '%s' "${CUR_JSON}" | jq -r --arg ip "$RUNNER_CIDR" '(. // []) + [$ip] | unique | join(",")')"

          echo "Applying EKS update: endpointPublicAccess=true, publicAccessCidrs=${NEW_CSV}"
          UPDATE_ID="$(aws eks update-cluster-config \
            --name "${CLUSTER_NAME}" \
            --resources-vpc-config "endpointPublicAccess=true,publicAccessCidrs=${NEW_CSV}" \
            --query 'update.id' --output text)"

          for _ in {1..30}; do
            PHASE="$(aws eks describe-update --name "${CLUSTER_NAME}" --update-id "${UPDATE_ID}" \
              --query 'update.status' --output text)"
            [[ "${PHASE}" == "Successful" ]] && break
            [[ "${PHASE}" == "Failed" ]] && { echo "EKS update failed"; exit 1; }
            sleep 10
          done

          ENDPOINT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query 'cluster.endpoint' --output text)"
          code="$(curl -k -sS -o /dev/null -w '%{http_code}' --connect-timeout 5 --max-time 10 "${ENDPOINT}/version" || echo 000)"
          if [[ "$code" == "200" || "$code" == "401" || "$code" == "403" ]]; then
            echo "API now reachable (HTTP ${code})."
          else
            echo "API still not reachable (HTTP ${code}). Aborting."
            exit 1
          fi

      - name: Ensure namespace
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f infrastructure/k8s/namespaces/nat20.yaml --request-timeout="${KUBECTL_TIMEOUT}"

      - name: Apply frontend Deployment/Service
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" apply -f infrastructure/k8s/frontend/deployment.yaml --request-timeout="${KUBECTL_TIMEOUT}"
          kubectl -n "${NAMESPACE}" apply -f infrastructure/k8s/frontend/service.yaml --request-timeout="${KUBECTL_TIMEOUT}"

      - name: Apply frontend Ingress
        shell: bash
        run: |
          set -euo pipefail
          export ORIGIN_CERT_ARN ALB_FRONTEND_SG_ID
          TMP="$(mktemp)"
          envsubst < infrastructure/k8s/ingress/frontend-ingress.yaml > "${TMP}"
          kubectl -n "${NAMESPACE}" apply -f "${TMP}" --request-timeout="${KUBECTL_TIMEOUT}"

      - name: Update image
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" set image deployment/frontend frontend="${IMAGE_URI}" --record
          kubectl -n "${NAMESPACE}" annotate deployment/frontend kubernetes.io/change-cause="kubectl set image to ${IMAGE_URI}" --overwrite

      - name: Wait for rollout
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" rollout status deployment/frontend --timeout=300s

      - name: "Ensure Route53 origin alias and persist ALB DNS"
        shell: bash
        run: |
          set -euo pipefail
          # Resolve domain name automatically from CloudFront distribution comment
          if [[ -z "${DOMAIN_NAME}" ]]; then
            CF_ID="$(aws cloudfront list-distributions --query "DistributionList.Items[?Comment=='Frontend SPA + API routing to ALBs'].Id" --output text 2>/dev/null || true)"
            if [[ -n "${CF_ID}" && "${CF_ID}" != "None" ]]; then
              DOMAIN_NAME="$(aws cloudfront get-distribution --id "${CF_ID}" --query 'Distribution.DistributionConfig.Aliases.Items[0]' --output text)"
            fi
          fi
          if [[ -z "${DOMAIN_NAME}" || "${DOMAIN_NAME}" == "None" ]]; then
            echo "DOMAIN_NAME could not be auto-detected. Set env DOMAIN_NAME explicitly."; exit 1
          fi

          ORIGIN_FQDN="origin.${DOMAIN_NAME}"
          HZ_ID="$(aws route53 list-hosted-zones-by-name --dns-name "${DOMAIN_NAME}" --query 'HostedZones[0].Id' --output text | sed 's|/hostedzone/||')"
          [[ -n "${HZ_ID}" ]] || { echo "Hosted zone not found for ${DOMAIN_NAME}"; exit 1; }

          # Ingress ALB hostname and its hosted zone id
          ALB_DNS="$(kubectl -n "${NAMESPACE}" get ingress frontend -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')"
          [[ -n "${ALB_DNS}" && "${ALB_DNS}" != "<no value>" ]] || { kubectl -n "${NAMESPACE}" describe ingress frontend || true; exit 1; }
          echo "FRONTEND_ALB_DNS=${ALB_DNS}" >> "$GITHUB_ENV"

          ALB_ZONE_ID="$(aws elbv2 describe-load-balancers --query "LoadBalancers[?DNSName=='${ALB_DNS}'].CanonicalHostedZoneId" --output text)"
          [[ -n "${ALB_ZONE_ID}" ]] || { echo "Could not resolve ALB HostedZoneId for ${ALB_DNS}"; exit 1; }

          # Persist in SSM for Terraform and future runs
          aws ssm put-parameter --name "/${PROJECT_NAME}/dns/FRONTEND_ALB_DNS" --type String --overwrite --value "${ALB_DNS}"

          # UPSERT A and AAAA aliases (use dualstack for IPv6 support)
          DUAL_DNS="dualstack.${ALB_DNS}"

          cat > /tmp/r53-origin.json <<JSON
          {
            "Comment": "Ensure origin alias to ALB",
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "${ORIGIN_FQDN}",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "${ALB_ZONE_ID}",
                    "DNSName": "${DUAL_DNS}",
                    "EvaluateTargetHealth": true
                  }
                }
              },
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "${ORIGIN_FQDN}",
                  "Type": "AAAA",
                  "AliasTarget": {
                    "HostedZoneId": "${ALB_ZONE_ID}",
                    "DNSName": "${DUAL_DNS}",
                    "EvaluateTargetHealth": true
                  }
                }
              }
            ]
          }
          JSON
          aws route53 change-resource-record-sets --hosted-zone-id "${HZ_ID}" --change-batch file:///tmp/r53-origin.json

      - name: Invalidate CloudFront (viewer cache)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          CF_ID="$(aws cloudfront list-distributions \
            --query "DistributionList.Items[?Comment=='Frontend SPA + API routing to ALBs'].Id | [0]" \
            --output text 2>/dev/null || true)"
          if [[ -z "${CF_ID}" || "${CF_ID}" == "None" ]]; then
            echo "Could not resolve CloudFront distribution ID; skip invalidation."
            exit 0
          fi
          echo "Invalidating CloudFront cache for distribution ${CF_ID}"
          aws cloudfront create-invalidation --distribution-id "${CF_ID}" --paths \
            "/" "/index.html" "/pages/*" "/scripts/*" "/styles/*"

      - name: "Cleanup temporary runner CIDR"
        if: always() && steps.preflight.outputs.ok != 'true'
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${RUNNER_CIDR:-}" ]]; then
            echo "No RUNNER_CIDR set; skipping cleanup."; exit 0
          fi
          CUR_JSON="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output json)"
          if [[ -z "${CUR_JSON}" || "${CUR_JSON}" == "null" ]]; then CUR_JSON='[]'; fi
          REST_CSV="$(printf '%s' "${CUR_JSON}" | jq -r --arg ip "$RUNNER_CIDR" '(. // []) | map(select(. != $ip)) | unique | join(",")')"

          echo "Reverting EKS publicAccessCidrs to: ${REST_CSV}"
          UPDATE_ID="$(aws eks update-cluster-config \
            --name "${CLUSTER_NAME}" \
            --resources-vpc-config "endpointPublicAccess=true,publicAccessCidrs=${REST_CSV}" \
            --query 'update.id' --output text)"

          for _ in {1..30}; do
            PHASE="$(aws eks describe-update --name "${CLUSTER_NAME}" --update-id "${UPDATE_ID}" --query 'update.status' --output text)"
            [[ "${PHASE}" == "Successful" ]] && break
            [[ "${PHASE}" == "Failed" ]] && { echo "Cleanup EKS update failed"; exit 1; }
            sleep 10
          done

      - name: "On failure: collect diagnostics"
        if: failure()
        shell: bash
        run: |
          set -euo pipefail
          echo "=== cluster nodes ==="
          kubectl get nodes -o wide || true

          echo "=== objects ==="
          kubectl -n "${NAMESPACE}" get all -o wide || true
          kubectl -n "${NAMESPACE}" describe deployment frontend || true
          kubectl -n "${NAMESPACE}" get rs -o wide || true
          kubectl -n "${NAMESPACE}" get ingress frontend -o yaml || true

          echo "=== recent events (unique lines with counts, last 300) ==="
          kubectl -n "${NAMESPACE}" get events --sort-by=.lastTimestamp | tail -n 300 \
            | awk '{c[$0]++} END {for (l in c) printf "%5d × %s\n", c[l], l}' | sort -nr | head -n 200 || true

          echo "=== frontend pod logs (last 400, non-probe grouped) ==="
          for p in $(kubectl -n "${NAMESPACE}" get pods -l app=frontend -o name | sed 's|pod/||'); do
            echo "--- $p ---"
            kubectl -n "${NAMESPACE}" logs "$p" --tail=400 2>/dev/null \
              | grep -v 'kube-probe/1' \
              | awk '{c[$0]++} END {for (l in c) printf "%5d × %s\n", c[l], l}' | sort -nr | head -n 200 || true
          done
