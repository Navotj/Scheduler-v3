name: Deploy Frontend to EKS

on:
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - "docker/frontend/**"
      - "frontend/**"
      - "infrastructure/k8s/frontend/**"
      - "infrastructure/k8s/ingress/frontend-ingress.yaml"
      - ".github/workflows/deploy_frontend.yml"

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: eu-central-1
  CLUSTER_NAME: nat20-eks
  PROJECT_NAME: nat20
  NAMESPACE: nat20
  ECR_REPO: nat20/frontend
  KUBECTL_TIMEOUT: 30s
  AWS_RETRY_MODE: standard
  AWS_MAX_ATTEMPTS: 6

jobs:
  deploy:
    name: deploy
    runs-on: ubuntu-24.04

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install jq/curl/envsubst
        shell: bash
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y jq curl gettext-base

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: gha-frontend-deploy

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & push image
        id: build
        shell: bash
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        run: |
          set -euo pipefail
          IMAGE_URI="${ECR_REGISTRY}/${ECR_REPO}:sha-${GITHUB_SHA}"
          echo "IMAGE_URI=${IMAGE_URI}" | tee -a "$GITHUB_ENV"
          docker buildx build \
            --push \
            --tag "${IMAGE_URI}" \
            -f docker/frontend/Dockerfile \
            .
          echo "pushed=${IMAGE_URI}"

      - name: Read network params from SSM
        id: read-ssm
        shell: bash
        run: |
          set -euo pipefail
          ORIGIN_CERT_ARN="$(aws ssm get-parameter --name /nat20/network/ORIGIN_CERT_ARN --query 'Parameter.Value' --output text)"
          ALB_FRONTEND_SG_ID="$(aws ssm get-parameter --name /nat20/network/ALB_FRONTEND_SG_ID --query 'Parameter.Value' --output text)"
          echo "ORIGIN_CERT_ARN=${ORIGIN_CERT_ARN}" >> "$GITHUB_ENV"
          echo "ALB_FRONTEND_SG_ID=${ALB_FRONTEND_SG_ID}" >> "$GITHUB_ENV"

      - name: Preflight: API reachability (brief, authorized)
        id: preflight
        shell: bash
        run: |
          set -euo pipefail
          # Try a very short, authorized probe using kubectl; no mutations.
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}" >/dev/null
          ok="false"
          for i in 1 2 3; do
            if kubectl --request-timeout=10s get --raw=/readyz >/dev/null 2>&1; then
              ok="true"; break
            fi
            sleep 5
          done
          echo "ok=${ok}" >> "$GITHUB_OUTPUT"
          echo "Preflight result: ${ok}"

      - name: Fallback: temporarily allow this runner CIDR (one attempt)
        if: steps.preflight.outputs.ok != 'true'
        id: cidr-fallback
        shell: bash
        run: |
          set -euo pipefail

          runner_ip="$(curl -s https://checkip.amazonaws.com | tr -d '\n\r')"
          [[ -n "${runner_ip}" ]]
          runner_cidr="${runner_ip}/32"

          current="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
                  --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output json)"
          updated="$(jq -cn --arg ip "${runner_cidr}" --argjson cur "${current:-null}" '((($cur // []) | map(tostring)) + [$ip]) | unique | join(",")')"

          echo "Applying EKS update: endpointPublicAccess=true, publicAccessCidrs=${updated}"
          update_id="$(aws eks update-cluster-config \
              --name "${CLUSTER_NAME}" \
              --region "${AWS_REGION}" \
              --resources-vpc-config "endpointPublicAccess=true,publicAccessCidrs=${updated}" \
              --query update.id --output text)"

          # Wait for update to finish (fast poll, hard 90s cap)
          for _ in {1..18}; do
            status="$(aws eks describe-update --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --update-id "${update_id}" --query 'update.status' --output text)"
            [[ "${status}" == "Failed" ]] && { echo "EKS update failed"; exit 1; }
            [[ "${status}" == "Successful" ]] && break
            sleep 5
          done

          # Re-check with authorized probe; fail fast if still not reachable.
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}" >/dev/null
          if ! kubectl --request-timeout=15s get --raw=/readyz >/dev/null 2>&1; then
            echo "Cluster API still not reachable after CIDR fallback." >&2
            exit 1
          fi

      - name: Connect & basic cluster info
        shell: bash
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}"
          kubectl version --client
          kubectl get nodes -o wide

      - name: Ensure namespace
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f infrastructure/k8s/namespaces/nat20.yaml

      - name: Apply frontend (deployment + service)
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" apply -f infrastructure/k8s/frontend/deployment.yaml
          kubectl -n "${NAMESPACE}" apply -f infrastructure/k8s/frontend/service.yaml

      - name: Set image on deployment
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" set image deployment/frontend frontend="${IMAGE_URI}" --record=true
          kubectl -n "${NAMESPACE}" annotate deployment/frontend kubernetes.io/change-cause="kubectl set image to ${IMAGE_URI}" --overwrite

      - name: Apply ingress
        shell: bash
        env:
          ORIGIN_CERT_ARN: ${{ env.ORIGIN_CERT_ARN }}
          ALB_FRONTEND_SG_ID: ${{ env.ALB_FRONTEND_SG_ID }}
        run: |
          set -euo pipefail
          export ORIGIN_CERT_ARN ALB_FRONTEND_SG_ID
          tmp="$(mktemp)"
          envsubst < infrastructure/k8s/ingress/frontend-ingress.yaml > "${tmp}"
          kubectl apply -f "${tmp}" --request-timeout="${KUBECTL_TIMEOUT}"

      - name: Wait for rollout
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n "${NAMESPACE}" rollout status deployment/frontend --timeout=300s

      - name: On failure: collect diagnostics
        if: failure()
        shell: bash
        run: |
          set -euo pipefail
          echo "=== k8s objects ==="
          kubectl -n "${NAMESPACE}" get all -l app=frontend -o wide || true

          echo "=== deployment describe ==="
          kubectl -n "${NAMESPACE}" describe deployment frontend || true

          echo "=== newest replicaset describe ==="
          RS="$(kubectl -n "${NAMESPACE}" get rs -l app=frontend -o json \
                | jq -r '.items | sort_by(.metadata.creationTimestamp) | last(.[]?).metadata.name // empty')"
          if [[ -n "${RS}" ]]; then
            kubectl -n "${NAMESPACE}" describe rs "${RS}" || true
          fi

          echo "=== recent events (tail 200) ==="
          kubectl -n "${NAMESPACE}" get events --sort-by=.lastTimestamp | tail -n 200 || true

          echo "=== pod logs (filtered) ==="
          mapfile -t PODS < <(kubectl -n "${NAMESPACE}" get pods -l app=frontend -o jsonpath='{.items[*].metadata.name}')
          for p in "${PODS[@]}"; do
            echo "--- logs for ${p} ---"
            kubectl -n "${NAMESPACE}" logs "${p}" --tail=500 2>&1 \
              | awk '/kube-probe/ {c++; next} {print} END{ if (c) printf("[elided %d kube-probe lines]\n", c) }' || true
            echo
          done

          exit 1
