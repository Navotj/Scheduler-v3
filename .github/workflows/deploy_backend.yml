name: Deploy Backend to EC2

on:
  push:
    branches: [ "main" ]
    paths:
      - "backend/**"
      - ".github/workflows/deploy_backend.yml"
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: eu-central-1

    - name: Derive artifact bucket name
      id: acct
      run: |
        ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
        echo "ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV
        ARTIFACT_BUCKET="nat20scheduling-com-deploy-artifacts-$ACCOUNT_ID"
        echo "ARTIFACT_BUCKET=$ARTIFACT_BUCKET" >> $GITHUB_ENV

    - name: Verify artifact bucket exists (managed by Terraform)
      run: |
        if ! aws s3api head-bucket --bucket "$ARTIFACT_BUCKET" 2>/dev/null; then
          echo "Artifact bucket $ARTIFACT_BUCKET does not exist. Run the Terraform workflow first." >&2
          exit 1
        fi

    - name: Get Backend Instance ID from tag
      id: backend_instance
      run: |
        INSTANCE_ID=$(aws ec2 describe-instances \
          --filters "Name=tag:Name,Values=terraform-backend" "Name=instance-state-name,Values=running" \
          --query "Reservations[0].Instances[0].InstanceId" \
          --output text)
        if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" = "None" ]; then
          echo "No running instance found with tag Name=terraform-backend" >&2
          exit 1
        fi
        echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_ENV
        echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT

    - name: Zip backend files (code only, no secrets)
      run: |
        cd backend
        zip -r ../backend.zip .
        cd ..

    - name: Upload artifact to private S3 (encrypted)
      run: |
        set -euo pipefail
        KEY="backend/${GITHUB_SHA}.zip"
        echo "ARTIFACT_KEY=$KEY" >> $GITHUB_ENV
        aws s3 cp backend.zip "s3://$ARTIFACT_BUCKET/$KEY" --sse AES256

    - name: Deploy on instance via SSM (download from S3, render .env from SSM)
      env:
        INSTANCE_ID: ${{ env.INSTANCE_ID }}
        ARTIFACT_BUCKET: ${{ env.ARTIFACT_BUCKET }}
        ARTIFACT_KEY: ${{ env.ARTIFACT_KEY }}
      run: |
        set -euo pipefail

        # 1) Kick off the remote commands
        CMD_ID=$(aws ssm send-command \
          --document-name "AWS-RunShellScript" \
          --instance-ids "$INSTANCE_ID" \
          --region eu-central-1 \
          --comment "Deploy backend app from private S3 artifact; secrets from SSM" \
          --parameters commands="[
            \"set -euo pipefail\",
            \"sudo mkdir -p /opt/app /tmp/installation\",
            \"sudo chown -R root:root /tmp/installation\",
            \"aws s3 cp s3://${ARTIFACT_BUCKET}/${ARTIFACT_KEY} /tmp/installation/backend.zip\",
            \"unzip -o /tmp/installation/backend.zip -d /tmp/installation/\",
            \"if [ -d /tmp/installation/app ]; then sudo mv /tmp/installation/app/* /opt/app/; else sudo mv /tmp/installation/* /opt/app/; fi\",
            \"# Move unit file no matter where it was in the zip\",
            \"if [ -f /tmp/installation/scheduler.service ]; then sudo mv /tmp/installation/scheduler.service /etc/systemd/system/scheduler.service; fi\",
            \"if [ -f /opt/app/scheduler.service ]; then sudo mv /opt/app/scheduler.service /etc/systemd/system/scheduler.service; fi\",
            \"# Fetch secrets from SSM (escape \\$ so it evaluates remotely, not on the GH runner)\",
            \"JWT_SECRET=\\$(aws ssm get-parameter --name /nat20/backend/JWT_SECRET --with-decryption --query Parameter.Value --output text)\",
            \"MONGO_USER=\\$(aws ssm get-parameter --name /nat20/mongo/USER --with-decryption --query Parameter.Value --output text)\",
            \"MONGO_PASS=\\$(aws ssm get-parameter --name /nat20/mongo/PASSWORD --with-decryption --query Parameter.Value --output text)\",
            \"MONGO_HOST=\\$(aws ssm get-parameter --name /nat20/mongo/HOST --with-decryption --query Parameter.Value --output text)\",
            \"MONGO_DB=\\$(aws ssm get-parameter --name /nat20/mongo/DB --with-decryption --query Parameter.Value --output text)\",
            \"# Write .env\",
            \"echo JWT_SECRET=\\$JWT_SECRET | sudo tee /opt/app/.env >/dev/null\",
            \"echo MONGO_URI=mongodb://\\$MONGO_USER:\\$MONGO_PASS@\\$MONGO_HOST:27017/\\$MONGO_DB?authSource=admin | sudo tee -a /opt/app/.env >/dev/null\",
            \"echo MONGO_DB_NAME=\\$MONGO_DB | sudo tee -a /opt/app/.env >/dev/null\",
            \"echo PORT=3000 | sudo tee -a /opt/app/.env >/dev/null\",
            \"echo COOKIE_SECURE=true | sudo tee -a /opt/app/.env >/dev/null\",
            \"cd /opt/app && npm install --omit=dev\",
            \"sudo systemctl daemon-reload\",
            \"sudo systemctl enable scheduler || true\",
            \"sudo systemctl restart scheduler || (journalctl -u scheduler --no-pager -n 200; exit 1)\",
            \"sleep 2; ( ss -lntp | sed -n '1,200p' || true )\",
            \"curl -sS -o /dev/null -w 'HTTP:%{http_code}\\n' http://127.0.0.1:3000/ || true\",
            \"rm -rf /tmp/installation\"
          ]" \
          --query "Command.CommandId" --output text)

        # 2) Poll for completion
        echo "Waiting for SSM command ${CMD_ID} to finish..."
        for i in $(seq 1 60); do
          STATUS=$(aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query Status --output text || echo "Pending")
          if [ "$STATUS" = "Success" ]; then
            break
          fi
          if [ "$STATUS" = "Failed" ] || [ "$STATUS" = "Cancelled" ] || [ "$STATUS" = "TimedOut" ]; then
            echo "SSM status: $STATUS"
            echo "SSM STDOUT"
            aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query 'StandardOutputContent' --output text || true
            echo
            echo "SSM STDERR"
            aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query 'StandardErrorContent' --output text || true
            echo
            echo "Remote deploy script failed with status: $STATUS" >&2
            exit 1
          fi
          sleep 5
        done

        # 3) Print logs on success too (handy for debugging)
        echo "SSM status: Success"
        echo "SSM STDOUT"
        aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query 'StandardOutputContent' --output text || true
        echo
        echo "SSM STDERR"
        aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query 'StandardErrorContent' --output text || true
        echo

    - name: Clean up local artifact
      if: always()
      run: rm -f backend.zip
