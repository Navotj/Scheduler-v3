name: Deploy Backend to EC2

on:
  push:
    branches: [ "main" ]
    paths:
      - "backend/**"
      - ".github/workflows/deploy_backend.yml"
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-central-1

      - name: Derive artifact bucket name
        id: acct
        run: |
          set -euo pipefail
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "ACCOUNT_ID=$ACCOUNT_ID" >> $GITHUB_ENV
          ARTIFACT_BUCKET="nat20scheduling-com-deploy-artifacts-$ACCOUNT_ID"
          echo "ARTIFACT_BUCKET=$ARTIFACT_BUCKET" >> $GITHUB_ENV

      - name: Verify artifact bucket exists (managed by Terraform)
        run: |
          set -euo pipefail
          if ! aws s3api head-bucket --bucket "$ARTIFACT_BUCKET" 2>/dev/null; then
            echo "Artifact bucket $ARTIFACT_BUCKET does not exist. Run the Terraform workflow first." >&2
            exit 1
          fi

      - name: Get Backend Instance ID from tag
        id: backend_instance
        run: |
          set -euo pipefail
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=terraform-backend" "Name=instance-state-name,Values=running" \
            --query "Reservations[0].Instances[0].InstanceId" \
            --output text)
          if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" = "None" ]; then
            echo "No running instance tagged Name=terraform-backend" >&2
            exit 1
          fi
          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_ENV
          echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT

      - name: Zip backend files (code only, no secrets)
        run: |
          set -euo pipefail
          cd backend
          zip -r ../backend.zip .
          cd ..

      - name: Upload artifact to private S3 (encrypted)
        run: |
          set -euo pipefail
          KEY="backend/${GITHUB_SHA}.zip"
          echo "ARTIFACT_KEY=$KEY" >> $GITHUB_ENV
          aws s3 cp backend.zip "s3://$ARTIFACT_BUCKET/$KEY" --sse AES256

      - name: Deploy on instance via SSM (download from S3, render .env from SSM)
        env:
          INSTANCE_ID: ${{ env.INSTANCE_ID }}
          ARTIFACT_BUCKET: ${{ env.ARTIFACT_BUCKET }}
          ARTIFACT_KEY: ${{ env.ARTIFACT_KEY }}
        run: |
          set -euo pipefail

          cat > /tmp/ssm_commands.txt <<'CMDS'
          set -euo pipefail

          # Ensure basic tools exist
          if ! command -v unzip >/dev/null 2>&1; then
            if command -v dnf >/dev/null 2>&1; then dnf install -y unzip; elif command -v yum >/dev/null 2>&1; then yum install -y unzip; elif command -v apt-get >/dev/null 2>&1; then apt-get update -y && apt-get install -y unzip; fi
          fi
          if ! command -v rsync >/dev/null 2>&1; then
            if command -v dnf >/dev/null 2>&1; then dnf install -y rsync; elif command -v yum >/dev/null 2>&1; then yum install -y rsync; elif command -v apt-get >/dev/null 2>&1; then apt-get update -y && apt-get install -y rsync; fi
          fi
          if ! command -v curl >/dev/null 2>&1; then
            if command -v dnf >/dev/null 2>&1; then dnf install -y curl; elif command -v yum >/dev/null 2>&1; then yum install -y curl; elif command -v apt-get >/dev/null 2>&1; then apt-get update -y && apt-get install -y curl; fi
          fi

          sudo mkdir -p /opt/app /tmp/installation
          sudo chown -R root:root /tmp/installation

          # Download and extract artifact (into a clean unpack dir)
          aws s3 cp "s3://${ARTIFACT_BUCKET}/${ARTIFACT_KEY}" /tmp/installation/backend.zip
          rm -rf /tmp/installation/unpack
          mkdir -p /tmp/installation/unpack
          unzip -o /tmp/installation/backend.zip -d /tmp/installation/unpack/

          # Choose source dir (handles archives that contain an /app folder)
          SRC="/tmp/installation/unpack"
          if [ -d "/tmp/installation/unpack/app" ]; then SRC="/tmp/installation/unpack/app"; fi

          # Ensure destination exists, then sync (idempotent; removes stale files)
          sudo mkdir -p /opt/app
          sudo rsync -a --delete "$SRC"/ /opt/app/

          # Move unit file if present (top-level or inside app)
          if [ -f /tmp/installation/unpack/scheduler.service ]; then
            sudo cp -f /tmp/installation/unpack/scheduler.service /etc/systemd/system/scheduler.service
          fi
          if [ -f /opt/app/scheduler.service ]; then
            sudo cp -f /opt/app/scheduler.service /etc/systemd/system/scheduler.service
          fi

          # Fetch secrets from SSM (evaluated on the instance)
          JWT_SECRET=$(aws ssm get-parameter --name /nat20/backend/JWT_SECRET --with-decryption --query Parameter.Value --output text)
          MONGO_USER=$(aws ssm get-parameter --name /nat20/mongo/USER --with-decryption --query Parameter.Value --output text)
          MONGO_PASS=$(aws ssm get-parameter --name /nat20/mongo/PASSWORD --with-decryption --query Parameter.Value --output text)
          MONGO_HOST=$(aws ssm get-parameter --name /nat20/mongo/HOST --with-decryption --query Parameter.Value --output text)
          MONGO_DB=$(aws ssm get-parameter --name /nat20/mongo/DB --with-decryption --query Parameter.Value --output text)

          # Write .env
          echo JWT_SECRET=$JWT_SECRET | sudo tee /opt/app/.env >/dev/null
          echo MONGO_URI=mongodb://$MONGO_USER:$MONGO_PASS@$MONGO_HOST:27017/$MONGO_DB?authSource=admin | sudo tee -a /opt/app/.env >/dev/null
          echo MONGO_DB_NAME=$MONGO_DB | sudo tee -a /opt/app/.env >/dev/null
          echo PORT=3000 | sudo tee -a /opt/app/.env >/dev/null
          echo COOKIE_SECURE=true | sudo tee -a /opt/app/.env >/dev/null

          # Ensure Node.js/npm is available for the root session used by SSM
          if ! command -v npm >/dev/null 2>&1; then
            if command -v dnf >/dev/null 2>&1; then
              curl -fsSL https://rpm.nodesource.com/setup_22.x | bash -
              dnf install -y nodejs
            elif command -v yum >/dev/null 2>&1; then
              curl -fsSL https://rpm.nodesource.com/setup_22.x | bash -
              yum install -y nodejs
            elif command -v apt-get >/dev/null 2>&1; then
              curl -fsSL https://deb.nodesource.com/setup_22.x | bash -
              apt-get update -y
              apt-get install -y nodejs
            else
              echo "Unsupported package manager: cannot install Node.js" >&2
              exit 1
            fi
          fi

          # Install deps (prefer lockfile if present)
          cd /opt/app
          if [ -f package-lock.json ]; then
            npm ci --omit=dev
          else
            npm install --omit=dev
          fi

          sudo systemctl daemon-reload
          sudo systemctl enable scheduler || true
          sudo systemctl restart scheduler || (journalctl -u scheduler --no-pager -n 200; exit 1)

          sleep 2; ss -lntp | sed -n "1,200p" || true
          curl -sS -o /dev/null -w 'HTTP:%{http_code}\n' http://127.0.0.1:3000/ || true
          rm -rf /tmp/installation
          CMDS

          # Convert to JSON array for SSM
          jq -Rs '{commands: (split("\n") | map(select(length>0)))}' /tmp/ssm_commands.txt > /tmp/ssm_params.json

          # Kick off the SSM command
          CMD_ID=$(aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --instance-ids "$INSTANCE_ID" \
            --region eu-central-1 \
            --comment "Deploy backend app from private S3 artifact; secrets from SSM" \
            --parameters file:///tmp/ssm_params.json \
            --query "Command.CommandId" --output text)

          # Poll for completion
          echo "Waiting for SSM command ${CMD_ID} to finish..."
          for i in $(seq 1 60); do
            STATUS=$(aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query Status --output text || echo "Pending")
            if [ "$STATUS" = "Success" ]; then
              break
            fi
            if [ "$STATUS" = "Failed" ] || [ "$STATUS" = "Cancelled" ] || [ "$STATUS" = "TimedOut" ]; then
              echo "SSM status: $STATUS"
              echo "SSM STDOUT"
              aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query 'StandardOutputContent' --output text || true
              echo
              echo "SSM STDERR"
              aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query 'StandardErrorContent' --output text || true
              echo
              echo "Remote deploy script failed with status: $STATUS" >&2
              exit 1
            fi
            sleep 5
          done

          # Print logs even on success
          echo "SSM status: Success"
          echo "SSM STDOUT"
          aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query 'StandardOutputContent' --output text || true
          echo
          echo "SSM STDERR"
          aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$INSTANCE_ID" --query 'StandardErrorContent' --output text || true
          echo

      - name: Clean up local artifact
        if: always()
        run: rm -f backend.zip
