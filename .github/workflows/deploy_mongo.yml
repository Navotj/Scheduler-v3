name: Deploy Mongo to EKS

on:
  push:
    branches: ["main"]
    paths:
      - ".github/workflows/deploy_mongo.yml"
      - "infrastructure/k8s/mongo/**"
      - "infrastructure/k8s/externalsecrets/mongo-secrets.yaml"
      - "infrastructure/k8s/secret-stores/clustersecretstore.yaml"
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: eu-central-1
      CLUSTER_NAME: nat20-eks
      # Optional: override per-run in workflow_dispatch inputs or repo/environment variable
      # Example: set to your runner IP/32 for tighter exposure during the job
      EKS_JOB_PUBLIC_CIDRS: 0.0.0.0/0

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials (v4)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # --- Open EKS API just enough, fast TCP probe, quick-fail ---
      - name: Resolve EKS endpoint & temporarily open EKS API
        shell: bash
        run: |
          set -euo pipefail
          export AWS_PAGER=""
          # Record original allowlist + endpoint
          ORIG="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
            --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output text | sed 's/\t/,/g')"
          EP="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
            --query 'cluster.endpoint' --output text)"
          echo "EKS_ENDPOINT=${EP}" >> "$GITHUB_ENV"
          echo "ORIGINAL_EKS_API_CIDRS=${ORIG}" >> "$GITHUB_ENV"

          # Open to requested allowlist (default 0.0.0.0/0)
          TARGET="${EKS_JOB_PUBLIC_CIDRS:-0.0.0.0/0}"
          if [ "${ORIG}" != "${TARGET}" ]; then
            set +e
            aws eks update-cluster-config --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
              --resources-vpc-config publicAccessCidrs="${TARGET}" >/dev/null 2>err.txt
            rc=$?
            set -e
            if [ $rc -ne 0 ] && ! grep -qi "already at the desired configuration" err.txt; then
              echo "EKS update failed:"; cat err.txt; exit 1
            fi
          fi

          # 3 quick TCP connection attempts (no long sleeps)
          HOST="$(echo "${EP}" | sed -E 's#https?://##g')"
          for i in 1 2 3; do
            if timeout 3 bash -lc "exec 3<>/dev/tcp/${HOST}/443" 2>/dev/null; then
              exec 3>&-
              echo "EKS endpoint reachable."
              exit 0
            fi
            sleep 2
          done
          echo "EKS endpoint NOT reachable after 3 tries."
          exit 1

      - name: Configure kubectl
        shell: bash
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}"

      - name: Ensure namespaces + PSS label (best-effort)
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns nat20 >/dev/null 2>&1 || kubectl create ns nat20
          kubectl get ns externalsecrets >/dev/null 2>&1 || kubectl create ns externalsecrets
          kubectl label ns nat20 \
            pod-security.kubernetes.io/enforce=restricted \
            pod-security.kubernetes.io/audit=restricted \
            pod-security.kubernetes.io/warn=restricted \
            --overwrite || true

      - name: Ensure External Secrets CRDs (server-side, pinned)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply --server-side -f https://raw.githubusercontent.com/external-secrets/external-secrets/v0.19.2/deploy/crds/bundle.yaml
          kubectl wait --for=condition=Established crd clustersecretstores.external-secrets.io --timeout=120s
          kubectl wait --for=condition=Established crd secretstores.external-secrets.io --timeout=120s
          kubectl wait --for=condition=Established crd externalsecrets.external-secrets.io --timeout=120s

      - name: Ensure EBS CSI driver add-on
        shell: bash
        run: |
          set -euo pipefail
          if ! aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver >/dev/null 2>&1; then
            aws eks create-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver
          else
            # Keep it fresh, but fast
            aws eks update-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver \
              --resolve-conflicts OVERWRITE >/dev/null || true
          fi
          echo "aws-ebs-csi-driver ACTIVE"

      - name: Ensure gp3 StorageClass
        shell: bash
        run: |
          set -euo pipefail
          cat <<'YAML' | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: gp3
          provisioner: ebs.csi.aws.com
          parameters:
            type: gp3
            fsType: ext4
            encrypted: "true"
          reclaimPolicy: Delete
          volumeBindingMode: WaitForFirstConsumer
          allowVolumeExpansion: true
          YAML
          kubectl get sc

      # --- No SSM calls here. External Secrets (IRSA) reads SSM in-cluster. ---
      - name: Apply ClusterSecretStore + Mongo ExternalSecret and wait for Secret
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f infrastructure/k8s/secret-stores/clustersecretstore.yaml
          kubectl apply -n nat20 -f infrastructure/k8s/externalsecrets/mongo-secrets.yaml

          # Wait up to ~90s for controller to create the kube Secret
          for i in {1..18}; do
            if kubectl -n nat20 get secret mongo-root >/dev/null 2>&1; then
              echo "mongo-root Secret present."
              break
            fi
            sleep 5
            if [ "$i" -eq 18 ]; then
              echo "mongo-root Secret not created in time."; exit 1
            fi
          done

      - name: Apply Mongo Services + StatefulSet (recreate on immutable drift)
        shell: bash
        run: |
          set -euo pipefail

          FILE="infrastructure/k8s/mongo/statefulset.yaml"

          # First try a normal apply (this also keeps Services in sync)
          set +e
          OUT="$(kubectl -n nat20 apply -f "${FILE}" 2>&1)"
          RC=$?
          set -e

          if [ $RC -ne 0 ] && echo "$OUT" | grep -q "StatefulSet .* is invalid: spec: Forbidden: updates to statefulset spec"; then
            echo "Detected immutable field change; recreating StatefulSet 'mongo'..."

            # Make sure Services exist before recreating
            kubectl -n nat20 get svc mongo-hl >/dev/null 2>&1 || kubectl -n nat20 apply -f "${FILE}"

            # Drain the STS cleanly, then delete only the STS object
            kubectl -n nat20 scale sts mongo --replicas=0 || true
            # Wait up to ~60s for pods to terminate
            for i in {1..30}; do
              CNT="$(kubectl -n nat20 get pods -l app=mongo --no-headers 2>/dev/null | wc -l | tr -d ' ')"
              [ "$CNT" = "0" ] && break
              sleep 2
            done
            kubectl -n nat20 delete sts mongo --wait=true || true

            # Recreate with the new immutable fields
            kubectl -n nat20 apply -f "${FILE}"
          else
            echo "$OUT"
          fi


      - name: Wait for PVC bound (fast retry, early diagnostics)
        shell: bash
        run: |
          set -euo pipefail
          # Give binder/provisioner up to ~2.5m with frequent checks
          for i in {1..30}; do
            PHASE="$(kubectl -n nat20 get pvc mongo-data-mongo-0 -o jsonpath='{.status.phase}' 2>/dev/null || true)"
            if [ "${PHASE}" = "Bound" ]; then
              echo "PVC bound."
              break
            fi
            # Quick diagnostic on common failure
            kubectl -n nat20 get pvc mongo-data-mongo-0 >/dev/null 2>&1 && kubectl -n nat20 describe pvc mongo-data-mongo-0 | tail -n +1 || true
            sleep 5
            if [ "$i" -eq 30 ]; then
              echo "PVC did not bind in time."; exit 1
            fi
          done

      - name: Wait for Mongo pod Ready (fast retry)
        shell: bash
        run: |
          set -euo pipefail
          for i in {1..36}; do
            READY="$(kubectl -n nat20 get pod -l app=mongo -o jsonpath='{.items[0].status.containerStatuses[0].ready}' 2>/dev/null || true)"
            if [ "${READY}" = "true" ]; then
              echo "Mongo pod Ready."
              break
            fi
            sleep 5
            if [ "$i" -eq 36 ]; then
              echo "Mongo pod not Ready in time."; kubectl -n nat20 get pods -o wide; kubectl -n nat20 describe pod -l app=mongo; exit 1
            fi
          done

      - name: Output Mongo service
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n nat20 get svc mongo -o wide || true

      # --- Always restore EKS API allowlist to original ---
      - name: Restore EKS API CIDR allowlist
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [ -n "${ORIGINAL_EKS_API_CIDRS:-}" ]; then
            set +e
            aws eks update-cluster-config --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
              --resources-vpc-config publicAccessCidrs="${ORIGINAL_EKS_API_CIDRS}" >/dev/null 2>err.txt
            rc=$?
            set -e
            if [ $rc -ne 0 ] && ! grep -qi "already at the desired configuration" err.txt; then
              echo "Restore of EKS API CIDRs failed (non-blocking):"; cat err.txt
            fi
          else
            echo "No ORIGINAL_EKS_API_CIDRS captured; leaving as-is."
          fi
