name: Deploy Mongo to EKS

on:
  workflow_dispatch: {}
  workflow_call: {}
  push:
    paths:
      - ".github/workflows/deploy_mongo.yml"
      - "infrastructure/k8s/mongo/**"
      - "infrastructure/k8s/externalsecrets/mongo-secrets.yaml"
      - "infrastructure/k8s/secret-stores/clustersecretstore.yaml"
      - "infrastructure/k8s/storageclasses/gp3.yaml"

permissions:
  id-token: write
  contents: read

concurrency:
  group: nat20-eks-ops
  cancel-in-progress: false

env:
  AWS_REGION: eu-central-1
  CLUSTER_NAME: nat20-eks
  PROJECT_NAME: nat20
  NAMESPACE: nat20
  KUBECTL_TIMEOUT: 45s
  AWS_RETRY_MODE: standard
  AWS_MAX_ATTEMPTS: 6

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Helm
        uses: azure/setup-helm@v4

      - name: Install jq + curl
        shell: bash
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y jq curl ca-certificates
          sudo update-ca-certificates

      - name: kubeconfig
        shell: bash
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}"
          kubectl version --client || true

      - name: Always allow this runner CIDR (idempotent) and wait ready
        id: allow-runner
        shell: bash
        run: |
          set -euo pipefail
          RUNNER_CIDR="$(curl -s https://checkip.amazonaws.com | tr -d '\r\n')/32"
          echo "RUNNER_CIDR=${RUNNER_CIDR}" >> "$GITHUB_ENV"

          CUR_JSON="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output json || echo '[]')"
          if [[ -z "${CUR_JSON}" || "${CUR_JSON}" == "null" ]]; then CUR_JSON='[]'; fi
          NEW_CSV="$(printf '%s' "${CUR_JSON}" | jq -r --arg ip "$RUNNER_CIDR" '(. // []) + [$ip] | unique | join(",")')"

          echo "Applying EKS update: publicAccessCidrs=${NEW_CSV}"
          UPDATE_ID="$(aws eks update-cluster-config \
            --name "${CLUSTER_NAME}" \
            --resources-vpc-config "endpointPublicAccess=true,publicAccessCidrs=${NEW_CSV}" \
            --query 'update.id' --output text)"

          for _ in {1..40}; do
            PHASE="$(aws eks describe-update --name "${CLUSTER_NAME}" --update-id "${UPDATE_ID}" --query 'update.status' --output text)"
            [[ "${PHASE}" == "Successful" ]] && break
            [[ "${PHASE}" == "Failed" ]] && { echo "EKS update failed"; exit 1; }
            sleep 5
          done

          ENDPOINT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query 'cluster.endpoint' --output text)"
          for i in 1 2 3 4 5; do
            code="$(curl -k -sS -o /dev/null -w '%{http_code}' --connect-timeout 5 --max-time 10 "${ENDPOINT}/version" || echo 000)"
            if [[ "$code" == "200" || "$code" == "401" || "$code" == "403" ]]; then
              echo "API reachable (HTTP ${code})."
              break
            fi
            sleep 5
          done

      - name: Ensure namespaces + PSS labels
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns "${NAMESPACE}" >/dev/null 2>&1 || kubectl create ns "${NAMESPACE}"
          kubectl get ns externalsecrets >/dev/null 2>&1 || kubectl create ns externalsecrets
          kubectl label ns "${NAMESPACE}" \
            pod-security.kubernetes.io/enforce=restricted \
            pod-security.kubernetes.io/audit=restricted \
            pod-security.kubernetes.io/warn=restricted \
            --overwrite || true

      - name: Ensure External Secrets Operator (Helm fallback)
        shell: bash
        run: |
          set -euo pipefail
          ACCOUNT_ID="$(aws sts get-caller-identity --query 'Account' --output text)"
          IRSA_ARN_ES="arn:aws:iam::${ACCOUNT_ID}:role/nat20-external-secrets-irsa"
          if ! kubectl -n externalsecrets get deploy external-secrets >/dev/null 2>&1; then
            helm repo add external-secrets https://charts.external-secrets.io
            helm repo update
            helm upgrade --install external-secrets external-secrets/external-secrets \
              --namespace externalsecrets --create-namespace \
              --set installCRDs=true \
              --set serviceAccount.create=true \
              --set serviceAccount.name=external-secrets \
              --set-string serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${IRSA_ARN_ES}"
            kubectl -n externalsecrets rollout status deploy/external-secrets --timeout=180s
          fi

      - name: Ensure EBS CSI driver (use Helm; purge conflicting add-on/resources)
        shell: bash
        run: |
          set -euo pipefail
          ACCOUNT_ID="$(aws sts get-caller-identity --query 'Account' --output text)"
          IRSA_ARN_EBS="arn:aws:iam::${ACCOUNT_ID}:role/nat20-ebs-csi-irsa"

          # Remove AWS managed add-on if present (to avoid conflicts)
          if aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver >/dev/null 2>&1; then
            ST="$(aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver --query 'addon.status' --output text || echo '')"
            echo "Existing EKS add-on status: ${ST}"
            aws eks delete-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver >/dev/null || true
            # Wait until gone
            for i in {1..60}; do
              aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver >/dev/null 2>&1 || { echo "Addon removed"; break; }
              sleep 5
            done
          fi

          # Purge any left-over resources that block Helm/add-on (immutable selector errors)
          kubectl -n kube-system delete ds/ebs-csi-node deploy/ebs-csi-controller --ignore-not-found
          kubectl -n kube-system delete all,sa,cm,secret -l app.kubernetes.io/name=aws-ebs-csi-driver --ignore-not-found
          kubectl delete clusterrole,clusterrolebinding -l app.kubernetes.io/name=aws-ebs-csi-driver --ignore-not-found

          # Install via Helm (stable and IRSA-enabled)
          helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
          helm repo update
          helm upgrade --install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver \
            --namespace kube-system \
            --set controller.serviceAccount.create=true \
            --set controller.serviceAccount.name=ebs-csi-controller-sa \
            --set-string controller.serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="${IRSA_ARN_EBS}"

          kubectl -n kube-system rollout status ds/ebs-csi-node --timeout=180s
          kubectl -n kube-system rollout status deploy/ebs-csi-controller --timeout=180s

      - name: Ensure gp3 StorageClass exists (default)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f infrastructure/k8s/storageclasses/gp3.yaml
          kubectl get sc gp3

      - name: Apply ClusterSecretStore + Mongo ExternalSecret
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f infrastructure/k8s/secret-stores/clustersecretstore.yaml
          kubectl apply -n "${NAMESPACE}" -f infrastructure/k8s/externalsecrets/mongo-secrets.yaml
          for i in {1..36}; do
            kubectl -n "${NAMESPACE}" get secret mongo-root >/dev/null 2>&1 && break
            sleep 5
          done
          kubectl -n "${NAMESPACE}" get secret mongo-root >/dev/null 2>&1 || { echo "mongo-root Secret not materialized"; exit 1; }

      - name: Deploy Mongo services + StatefulSet (immutable-safe)
        shell: bash
        run: |
          set -euo pipefail
          MANIFEST="infrastructure/k8s/mongo/statefulset.yaml"

          RAW_JSON="$(kubectl apply --dry-run=client -f "${MANIFEST}" -o json)"
          ITEMS_JSON="$(echo "${RAW_JSON}" | jq -c 'if .kind=="List" then .items else [.] end')"

          echo "${ITEMS_JSON}" | jq -c '.[] | select(.kind=="Service")' | while read -r SVC; do
            echo "${SVC}" | kubectl -n "${NAMESPACE}" apply -f -
          done

          DESIRED_STS="$(echo "${ITEMS_JSON}" | jq '.[] | select(.kind=="StatefulSet")')"
          [[ -n "${DESIRED_STS}" && "${DESIRED_STS}" != "null" ]]

          DES_SVC_NAME="$(echo "${DESIRED_STS}" | jq -r '.spec.serviceName')"
          DES_SELECTOR="$(echo "${DESIRED_STS}" | jq -c '.spec.selector')"
          DES_VCT="$(echo "${DESIRED_STS}" | jq -c '.spec.volumeClaimTemplates')"

          RECREATE=0
          if kubectl -n "${NAMESPACE}" get sts mongo >/dev/null 2>&1; then
            CUR_JSON="$(kubectl -n "${NAMESPACE}" get sts mongo -o json)"
            CUR_SVC_NAME="$(echo "${CUR_JSON}" | jq -r '.spec.serviceName')"
            CUR_SELECTOR="$(echo "${CUR_JSON}" | jq -c '.spec.selector')"
            CUR_VCT="$(echo "${CUR_JSON}" | jq -c '.spec.volumeClaimTemplates')"
            if [[ "${CUR_SVC_NAME}" != "${DES_SVC_NAME}" || "${CUR_SELECTOR}" != "${DES_SELECTOR}" || "${CUR_VCT}" != "${DES_VCT}" ]]; then
              RECREATE=1
            fi
          fi

          if [[ $RECREATE -eq 1 ]]; then
            echo "Detected immutable field change; recreating StatefulSet 'mongo'..."
            kubectl -n "${NAMESPACE}" scale sts mongo --replicas=0 || true
            kubectl -n "${NAMESPACE}" delete sts mongo --wait=true
          fi

          echo "${DESIRED_STS}" | kubectl -n "${NAMESPACE}" apply -f -

      - name: Wait for PVC bound and Pod ready
        shell: bash
        run: |
          set -euo pipefail
          for i in {1..30}; do
            PHASE="$(kubectl -n "${NAMESPACE}" get pvc mongo-data-mongo-0 -o jsonpath='{.status.phase}' 2>/dev/null || echo '')"
            if [[ "${PHASE}" == "Bound" ]]; then
              echo "PVC Bound."; break
            fi
            [[ $i -eq 30 ]] && { echo "PVC didn't bind in time."; kubectl -n "${NAMESPACE}" describe pvc mongo-data-mongo-0 || true; exit 1; }
            sleep 4
          done
          kubectl -n "${NAMESPACE}" rollout status statefulset/mongo --timeout=8m

      - name: Output Mongo service and pods
        shell: bash
        run: |
          set -euo pipefail
          kubectl get sc gp3
          kubectl -n "${NAMESPACE}" get pvc -l app=mongo -o wide || true
          kubectl -n "${NAMESPACE}" get svc mongo -o wide || true
          kubectl -n "${NAMESPACE}" get pods -l app=mongo -o wide || true

      - name: Cleanup temporary runner CIDR
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [[ -z "${RUNNER_CIDR:-}" ]]; then
            echo "No RUNNER_CIDR set; skipping cleanup."; exit 0
          fi
          CUR_JSON="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output json || echo '[]')"
          if [[ -z "${CUR_JSON}" || "${CUR_JSON}" == "null" ]]; then CUR_JSON='[]'; fi
          REST_CSV="$(printf '%s' "${CUR_JSON}" | jq -r --arg ip "$RUNNER_CIDR" '(. // []) | map(select(. != $ip)) | unique | join(",")')"
          echo "Reverting EKS publicAccessCidrs to: ${REST_CSV}"
          UPDATE_ID="$(aws eks update-cluster-config \
            --name "${CLUSTER_NAME}" \
            --resources-vpc-config "endpointPublicAccess=true,publicAccessCidrs=${REST_CSV}" \
            --query 'update.id' --output text)"
          for _ in {1..30}; do
            PHASE="$(aws eks describe-update --name "${CLUSTER_NAME}" --update-id "${UPDATE_ID}" --query 'update.status' --output text)"
            [[ "${PHASE}" == "Successful" ]] && break
            [[ "${PHASE}" == "Failed" ]] && { echo "Cleanup EKS update failed"; exit 1; }
            sleep 5
          done

      - name: "On failure: collect diagnostics (fast, gated)"
        if: failure()
        shell: bash
        env:
          MAX_TRIES: 5
        run: |
          set -euo pipefail
          ENDPOINT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query 'cluster.endpoint' --output text 2>/dev/null || true)"
          ok="false"
          for i in $(seq 1 "${MAX_TRIES}"); do
            code="$(curl -k -sS -o /dev/null -w '%{http_code}' --connect-timeout 3 --max-time 5 "${ENDPOINT}/version" || echo 000)"
            if [[ "$code" == "200" || "$code" == "401" || "$code" == "403" ]]; then ok="true"; break; fi
            sleep 2
          done
          if [[ "${ok}" != "true" ]]; then
            echo "Skipping diagnostics: API unreachable after ${MAX_TRIES} quick checks."
            exit 0
          fi
          RT="${KUBECTL_TIMEOUT:-10s}"
          echo "=== nodes ==="
          kubectl --request-timeout="${RT}" get nodes -o wide || true
          echo "=== ns objects (nat20) ==="
          kubectl -n "${NAMESPACE}" --request-timeout="${RT}" get all -o wide || true
          echo "=== events (grouped, last 300) ==="
          kubectl -n "${NAMESPACE}" --request-timeout="${RT}" get events --sort-by=.lastTimestamp | tail -n 300 || true
          echo "=== sts describe ==="
          kubectl -n "${NAMESPACE}" --request-timeout="${RT}" describe sts mongo || true
          echo "=== pod logs (last 200) ==="
          for p in $(kubectl -n "${NAMESPACE}" --request-timeout="${RT}" get pods -l app=mongo -o name 2>/dev/null | sed 's|pod/||'); do
            echo "--- $p ---"
            kubectl -n "${NAMESPACE}" --request-timeout="${RT}" logs "$p" --tail=200 2>/dev/null || true
          done
