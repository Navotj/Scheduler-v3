name: Deploy Mongo to EKS

on:
  push:
    branches: ["main"]
    paths:
      - ".github/workflows/deploy_mongo.yml"
      - "infrastructure/k8s/mongo/**"
      - "infrastructure/k8s/externalsecrets/mongo-secrets.yaml"
      - "infrastructure/k8s/secret-stores/clustersecretstore.yaml"
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: eu-central-1
      CLUSTER_NAME: nat20-eks
      # Temporary widen during job; restored at the end:
      EKS_JOB_PUBLIC_CIDRS: 0.0.0.0/0

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Capture current EKS API CIDRs and endpoint (safe outputs)
        shell: bash
        run: |
          set -euo pipefail
          ENDPOINT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --query 'cluster.endpoint' --output text)"
          echo "EKS_ENDPOINT=${ENDPOINT}" >> "$GITHUB_ENV"

          CUR_JSON="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
            --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output json || echo '[]')"
          ORIG_B64="$(printf '%s' "${CUR_JSON}" | base64 -w0)"
          echo "EKS_ORIG_CIDRS_B64=${ORIG_B64}" >> "$GITHUB_ENV"

      - name: Open EKS API temporarily and quick probe (3Ã— fast retries)
        shell: bash
        run: |
          set -euo pipefail
          TARGET="${EKS_JOB_PUBLIC_CIDRS:-0.0.0.0/0}"

          CUR_JSON="$(printf '%s' "${EKS_ORIG_CIDRS_B64:-}" | base64 -d 2>/dev/null || echo '[]')"
          CUR_JOIN="$(printf '%s' "${CUR_JSON}" | jq -r 'join(",")')"

          if [ -z "${CUR_JOIN}" ] || [ "${CUR_JOIN}" != "${TARGET}" ]; then
            set +e
            aws eks update-cluster-config --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
              --resources-vpc-config publicAccessCidrs="${TARGET}" >/dev/null 2>err.txt
            rc=$?
            set -e
            if [ $rc -ne 0 ] && ! grep -qi "already at the desired configuration" err.txt; then
              echo "Failed to widen EKS API CIDRs:"; cat err.txt; exit 1
            fi
          fi

          HOST="$(echo "${EKS_ENDPOINT}" | sed -E 's#https?://##')"
          for i in 1 2 3; do
            if timeout 3 bash -lc "exec 3<>/dev/tcp/${HOST}/443" 2>/dev/null; then
              exec 3>&-
              echo "EKS endpoint reachable."
              exit 0
            fi
            sleep 2
          done
          echo "EKS endpoint NOT reachable after 3 tries."
          exit 1

      - name: Configure kubectl
        shell: bash
        run: |
          set -euo pipefail
          aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}"

      - name: Ensure namespaces + PSS labels
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns nat20 >/dev/null 2>&1 || kubectl create ns nat20
          kubectl get ns externalsecrets >/dev/null 2>&1 || kubectl create ns externalsecrets
          kubectl label ns nat20 \
            pod-security.kubernetes.io/enforce=restricted \
            pod-security.kubernetes.io/audit=restricted \
            pod-security.kubernetes.io/warn=restricted \
            --overwrite || true

      - name: Ensure External Secrets CRDs (server-side)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply --server-side -f https://raw.githubusercontent.com/external-secrets/external-secrets/v0.19.2/deploy/crds/bundle.yaml
          kubectl wait --for=condition=Established crd clustersecretstores.external-secrets.io --timeout=120s
          kubectl wait --for=condition=Established crd secretstores.external-secrets.io --timeout=120s
          kubectl wait --for=condition=Established crd externalsecrets.external-secrets.io --timeout=120s

      - name: Ensure aws-ebs-csi-driver add-on is ACTIVE
        shell: bash
        run: |
          set -euo pipefail
          if ! aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver >/dev/null 2>&1; then
            aws eks create-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver --resolve-conflicts OVERWRITE >/dev/null
          else
            aws eks update-addon  --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver --resolve-conflicts OVERWRITE >/dev/null || true
          fi
          for i in {1..60}; do
            ST=$(aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --addon-name aws-ebs-csi-driver --query 'addon.status' --output text || echo "")
            [ "${ST}" = "ACTIVE" ] && { echo "aws-ebs-csi-driver ACTIVE"; break; }
            [ $i -eq 60 ] && { echo "aws-ebs-csi-driver not ACTIVE in time (last=${ST})"; exit 1; }
            sleep 5
          done

      - name: Grant EBS CSI creds via EKS Pod Identity (and restart driver)
        shell: bash
        env:
          EKS_EBS_ROLE_NAME: nat20-eks-ebs-csi-pod-id
        run: |
          set -euo pipefail
          CLUSTER="${CLUSTER_NAME}"
          REGION="${AWS_REGION}"
          ACC_ID="$(aws sts get-caller-identity --query Account --output text)"
          ROLE_NAME="${EKS_EBS_ROLE_NAME:-${CLUSTER}-ebs-csi-pod-id}"
          POLICY_ARN="arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"

          # Ensure pod identity agent
          if ! aws eks describe-addon --cluster-name "$CLUSTER" --addon-name eks-pod-identity-agent >/dev/null 2>&1; then
            aws eks create-addon --cluster-name "$CLUSTER" --addon-name eks-pod-identity-agent --resolve-conflicts OVERWRITE >/dev/null
          else
            aws eks update-addon  --cluster-name "$CLUSTER" --addon-name eks-pod-identity-agent --resolve-conflicts OVERWRITE >/dev/null || true
          fi
          for i in {1..60}; do
            ST=$(aws eks describe-addon --cluster-name "$CLUSTER" --addon-name eks-pod-identity-agent --query 'addon.status' --output text || echo "")
            [ "${ST}" = "ACTIVE" ] && { echo "eks-pod-identity-agent ACTIVE"; break; }
            [ $i -eq 60 ] && { echo "eks-pod-identity-agent not ACTIVE in time (last=${ST})"; exit 1; }
            sleep 5
          done

          # Create/Update role with MINIMAL Pod Identity trust policy (no conditions)
          TRUST_JSON="$(jq -n '{
            Version: "2012-10-17",
            Statement: [{
              Sid: "AllowEksAuthToAssumeRoleForPodIdentity",
              Effect: "Allow",
              Principal: { Service: "pods.eks.amazonaws.com" },
              Action: ["sts:AssumeRole","sts:TagSession"]
            }]
          }')"
          printf '%s' "$TRUST_JSON" > trust.json

          if ! aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
            aws iam create-role --role-name "$ROLE_NAME" --assume-role-policy-document file://trust.json >/dev/null
          fi
          # Always enforce the latest trust policy
          aws iam update-assume-role-policy --role-name "$ROLE_NAME" --policy-document file://trust.json >/dev/null

          # Attach policy (idempotent)
          HAS=$(aws iam list-attached-role-policies --role-name "$ROLE_NAME" \
               --query "AttachedPolicies[?PolicyArn=='${POLICY_ARN}'] | length(@)" --output text || echo 0)
          if [ "${HAS}" != "1" ]; then
            aws iam attach-role-policy --role-name "$ROLE_NAME" --policy-arn "$POLICY_ARN" >/dev/null
          fi
          ROLE_ARN="$(aws iam get-role --role-name "$ROLE_NAME" --query 'Role.Arn' --output text)"

          # Associate to controller SA
          ASSOC_ID="$(aws eks list-pod-identity-associations --cluster-name "$CLUSTER" \
            --query "associations[?namespace=='kube-system' && serviceAccount=='ebs-csi-controller-sa'].associationId" \
            --output text || true)"
          if [ -z "$ASSOC_ID" ]; then
            aws eks create-pod-identity-association --cluster-name "$CLUSTER" \
              --namespace kube-system --service-account ebs-csi-controller-sa --role-arn "$ROLE_ARN" >/dev/null
          else
            aws eks update-pod-identity-association --cluster-name "$CLUSTER" \
              --association-id "$ASSOC_ID" --role-arn "$ROLE_ARN" >/dev/null
          fi

          # Restart driver to pick up creds + topology
          kubectl -n kube-system rollout restart deploy ebs-csi-controller || true
          kubectl -n kube-system rollout restart ds ebs-csi-node || true

          # Wait for the EBS CSI pods to be Ready
          for i in {1..30}; do
            DS_READY=$(kubectl -n kube-system get ds ebs-csi-node -o jsonpath='{.status.numberReady}' 2>/dev/null || echo 0)
            DS_DESIRED=$(kubectl -n kube-system get ds ebs-csi-node -o jsonpath='{.status.desiredNumberScheduled}' 2>/dev/null || echo 0)
            DEP_READY=$(kubectl -n kube-system get deploy ebs-csi-controller -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo 0)
            if [ "$DS_READY" = "$DS_DESIRED" ] && [ "$DS_READY" != "0" ] && [ "${DEP_READY:-0}" != "0" ]; then
              echo "EBS CSI node/controller ready."
              break
            fi
            [ $i -eq 30 ] && { echo "EBS CSI driver not ready; abort."; kubectl -n kube-system get pods -l app.kubernetes.io/name=aws-ebs-csi-driver -o wide || true; exit 1; }
            sleep 2
          done

      - name: Wait for CSINode registration (per node) with topology keys
        shell: bash
        run: |
          set -euo pipefail
          NODES=$(kubectl get nodes -o name | sed 's#node/##')
          if [ -z "$NODES" ]; then
            echo "No nodes found"; exit 1
          fi
          for NODE in $NODES; do
            echo "Waiting for CSINode $NODE to register ebs.csi.aws.com with topology..."
            OK=0
            for i in {1..60}; do
              NAME=$(kubectl get csinode "$NODE" -o jsonpath='{.spec.drivers[?(@.name=="ebs.csi.aws.com")].name}' 2>/dev/null || echo "")
              KEYS=$(kubectl get csinode "$NODE" -o jsonpath='{.spec.drivers[?(@.name=="ebs.csi.aws.com")].topologyKeys}' 2>/dev/null || echo "")
              if [ "$NAME" = "ebs.csi.aws.com" ] && (echo "$KEYS" | grep -q "topology.kubernetes.io/zone\|topology.ebs.csi.aws.com/zone"); then
                OK=1; break
              fi
              sleep 2
            done
            [ $OK -eq 1 ] || { echo "CSINode $NODE not ready with topology keys"; kubectl get csinode "$NODE" -o yaml || true; exit 1; }
          done
          echo "All CSINode objects are ready."

      - name: Ensure gp3 StorageClass exists (no heredoc)
        shell: bash
        run: |
          set -euo pipefail
          if ! kubectl get sc gp3 >/dev/null 2>&1; then
            printf '%s\n' \
              'apiVersion: storage.k8s.io/v1' \
              'kind: StorageClass' \
              'metadata:' \
              '  name: gp3' \
              'provisioner: ebs.csi.aws.com' \
              'volumeBindingMode: WaitForFirstConsumer' \
              'allowVolumeExpansion: true' \
              'parameters:' \
              '  type: gp3' \
              '  fsType: ext4' \
              '  encrypted: "true"' \
            | kubectl apply -f -
          fi
          kubectl get sc gp3

      - name: Apply ClusterSecretStore + Mongo ExternalSecret
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f infrastructure/k8s/secret-stores/clustersecretstore.yaml
          kubectl apply -n nat20 -f infrastructure/k8s/externalsecrets/mongo-secrets.yaml
          for i in {1..36}; do
            if kubectl -n nat20 get secret mongo-root >/dev/null 2>&1; then
              echo "mongo-root Secret present."
              break
            fi
            [ $i -eq 36 ] && { echo "mongo-root Secret missing; abort."; exit 1; }
            sleep 5
          done

      - name: Reset stuck Mongo PVC (safe)
        shell: bash
        run: |
          set -euo pipefail
          kubectl -n nat20 delete pvc mongo-data-mongo-0 --ignore-not-found

      - name: Deploy Mongo services + StatefulSet (immutable-safe)
        shell: bash
        run: |
          set -euo pipefail
          MANIFEST="infrastructure/k8s/mongo/statefulset.yaml"

          RAW_JSON="$(kubectl apply --dry-run=client -f "${MANIFEST}" -o json)"
          ITEMS_JSON="$(echo "${RAW_JSON}" | jq -c 'if .kind=="List" then .items else [.] end')"

          # Apply Service objects first
          echo "${ITEMS_JSON}" | jq -c '.[] | select(.kind=="Service")' | while read -r SVC; do
            echo "${SVC}" | kubectl -n nat20 apply -f -
          done

          # Extract StatefulSet object
          DESIRED_STS="$(echo "${ITEMS_JSON}" | jq '.[] | select(.kind=="StatefulSet")')"
          [ -n "${DESIRED_STS}" ] && [ "${DESIRED_STS}" != "null" ] || { echo "ERROR: No StatefulSet in ${MANIFEST}"; exit 1; }

          DES_SVC_NAME="$(echo "${DESIRED_STS}" | jq -r '.spec.serviceName')"
          DES_SELECTOR="$(echo "${DESIRED_STS}" | jq -c '.spec.selector')"
          DES_VCT="$(echo "${DESIRED_STS}" | jq -c '.spec.volumeClaimTemplates')"

          RECREATE=0
          if kubectl -n nat20 get sts mongo >/dev/null 2>&1; then
            CUR_JSON="$(kubectl -n nat20 get sts mongo -o json)"
            CUR_SVC_NAME="$(echo "${CUR_JSON}" | jq -r '.spec.serviceName')"
            CUR_SELECTOR="$(echo "${CUR_JSON}" | jq -c '.spec.selector')"
            CUR_VCT="$(echo "${CUR_JSON}" | jq -c '.spec.volumeClaimTemplates')"
            if [ "${CUR_SVC_NAME}" != "${DES_SVC_NAME}" ] || [ "${CUR_SELECTOR}" != "${DES_SELECTOR}" ] || [ "${CUR_VCT}" != "${DES_VCT}" ]; then
              RECREATE=1
            fi
          fi

          if [ $RECREATE -eq 1 ]; then
            echo "Detected immutable field change; recreating StatefulSet 'mongo'..."
            kubectl -n nat20 scale sts mongo --replicas=0 || true
            for i in {1..60}; do
              CNT="$(kubectl -n nat20 get pods -l app=mongo --no-headers 2>/dev/null | wc -l | tr -d ' ')"
              [ "${CNT}" = "0" ] && break
              sleep 2
            done
            kubectl -n nat20 delete sts mongo --wait=true
          fi

          echo "${DESIRED_STS}" | kubectl -n nat20 apply -f -

      - name: Wait for PVC bound and Pod ready (fail-fast)
        shell: bash
        run: |
          set -euo pipefail
          for i in {1..30}; do
            PHASE="$(kubectl -n nat20 get pvc mongo-data-mongo-0 -o jsonpath='{.status.phase}' 2>/dev/null || echo '')"
            if [ "${PHASE}" = "Bound" ]; then
              echo "PVC Bound."
              break
            fi
            [ $i -eq 30 ] && { echo "PVC didn't bind fast enough."; kubectl -n nat20 describe pvc mongo-data-mongo-0 || true; exit 1; }
            sleep 4
          done
          kubectl -n nat20 rollout status statefulset/mongo --timeout=6m

      - name: Output Mongo service and pod
        shell: bash
        run: |
          set -euo pipefail
          kubectl get sc gp3
          kubectl -n nat20 get pvc mongo-data-mongo-0 -o wide || true
          kubectl -n nat20 get svc mongo -o wide || true
          kubectl -n nat20 get pods -l app=mongo -o wide || true

      - name: Restore original EKS API CIDRs
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          ORIG_JSON="$(printf '%s' "${EKS_ORIG_CIDRS_B64:-}" | base64 -d 2>/dev/null || echo '[]')"
          WANT="$(printf '%s' "${ORIG_JSON}" | jq -r 'join(",")')"
          if [ -n "${WANT}" ]; then
            aws eks update-cluster-config --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
              --resources-vpc-config publicAccessCidrs="${WANT}" >/dev/null || true
          fi
