name: Deploy Mongo to EKS

on:
  push:
    branches: ["main"]
    paths:
      - ".github/workflows/deploy_mongo.yml"
      - "infrastructure/k8s/mongo/**"
      - "infrastructure/k8s/externalsecrets/mongo-secrets.yaml"
      - "infrastructure/k8s/secret-stores/clustersecretstore.yaml"
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: eu-central-1
      CLUSTER_NAME: nat20-eks

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Capture current EKS API CIDRs + endpoint
        shell: bash
        run: |
          set -euo pipefail
          ORIG="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
            --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output text | sed 's/\t/,/g')"
          ENDPOINT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
            --query 'cluster.endpoint' --output text)"
          echo "ORIGINAL_EKS_API_CIDRS=${ORIG}" >> "$GITHUB_ENV"
          echo "EKS_ENDPOINT=${ENDPOINT}" >> "$GITHUB_ENV"

      - name: Temporarily allow runner IP for EKS API (union + wait)
        shell: bash
        run: |
          set -euo pipefail
          RUNNER_IP="$(curl -s https://checkip.amazonaws.com)/32"
          CURRENT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
            --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output text | sed 's/\t/,/g')"
          if [ -n "${CURRENT}" ] && [ "${CURRENT}" != "None" ]; then
            CIDRS="${RUNNER_IP},${CURRENT}"
          else
            CIDRS="${RUNNER_IP}"
          fi
          CIDRS="$(tr ',' '\n' <<< "${CIDRS}" | awk 'NF{print $0}' | sort -u | paste -sd, -)"
          if [ "${CURRENT}" != "${CIDRS}" ]; then
            UPDATE_JSON="$(aws eks update-cluster-config --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
              --resources-vpc-config publicAccessCidrs="${CIDRS}" --output json 2>/tmp/eks_update.err || true)"
            if [ -z "${UPDATE_JSON}" ]; then
              if ! grep -qi "already at the desired configuration" /tmp/eks_update.err; then
                cat /tmp/eks_update.err >&2; exit 1
              fi
            else
              UPDATE_ID="$(jq -r '.update.id' <<<"${UPDATE_JSON}")"
              for i in $(seq 1 60); do
                STATUS="$(aws eks describe-update --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
                  --update-id "${UPDATE_ID}" --query 'update.status' --output text || true)"
                [ "${STATUS}" = "Successful" ] && break
                if [ "${STATUS}" = "Failed" ] || [ "${STATUS}" = "Cancelled" ]; then
                  aws eks describe-update --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
                    --update-id "${UPDATE_ID}" --output json >&2
                  exit 1
                fi
                sleep 5
              done
            fi
          fi
          HOST="$(echo "${EKS_ENDPOINT}" | sed -E 's#https?://##g')"
          for i in $(seq 1 60); do
            if timeout 5 bash -lc "exec 3<>/dev/tcp/${HOST}/443" 2>/dev/null; then
              exec 3>&-
              break
            fi
            sleep 2
          done

      - name: Configure kubectl (with retries)
        shell: bash
        run: |
          set -euo pipefail
          for i in 1 2 3; do
            if aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${AWS_REGION}"; then
              break
            fi
            sleep $((i*5))
            [ $i -eq 3 ] && exit 1
          done
          for i in $(seq 1 30); do
            if kubectl --request-timeout=10s version --short >/dev/null 2>&1; then
              break
            fi
            sleep 3
          done

      - name: Ensure namespaces exist + enforce PSS (restricted)
        shell: bash
        run: |
          set -euo pipefail
          kubectl get ns nat20 >/dev/null 2>&1 || kubectl create ns nat20
          kubectl get ns externalsecrets >/dev/null 2>&1 || kubectl create ns externalsecrets
          kubectl label ns nat20 \
            pod-security.kubernetes.io/enforce=restricted \
            pod-security.kubernetes.io/audit=restricted \
            pod-security.kubernetes.io/warn=restricted \
            --overwrite || true

      - name: Ensure External Secrets CRDs (server-side, pinned)
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply --server-side -f https://raw.githubusercontent.com/external-secrets/external-secrets/v0.19.2/deploy/crds/bundle.yaml
          kubectl wait --for=condition=Established crd clustersecretstores.external-secrets.io --timeout=180s
          kubectl wait --for=condition=Established crd secretstores.external-secrets.io --timeout=180s
          kubectl wait --for=condition=Established crd externalsecrets.external-secrets.io --timeout=180s

      - name: Preflight check required SSM params
        shell: bash
        run: |
          set -euo pipefail
          MISSING=0
          for KEY in /nat20/mongo/USER /nat20/mongo/PASSWORD; do
            if ! aws ssm get-parameter --name "$KEY" >/dev/null 2>&1; then
              echo "Missing SSM parameter: $KEY" >&2; MISSING=1
            fi
          done
          [ $MISSING -eq 0 ] || { echo "Required SSM parameters missing; aborting." >&2; exit 1; }

      - name: Ensure AWS EBS CSI driver add-on (IRSA + add-on)
        shell: bash
        run: |
          set -euo pipefail
          export AWS_PAGER=""
          ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
          OIDC_ISSUER="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" --query 'cluster.identity.oidc.issuer' --output text)"
          OIDC_HOST="${OIDC_ISSUER#https://}"

          # Find matching IAM OIDC provider ARN
          PROVIDER_ARN=""
          for ARN in $(aws iam list-open-id-connect-providers --query 'OpenIDConnectProviderList[].Arn' --output text); do
            URL="$(aws iam get-open-id-connect-provider --open-id-connect-provider-arn "$ARN" --query 'Url' --output text 2>/dev/null || true)"
            if [ "${URL}" = "${OIDC_HOST}" ]; then
              PROVIDER_ARN="$ARN"; break
            fi
          done
          [ -n "${PROVIDER_ARN}" ] || { echo "ERROR: Cluster OIDC provider not found in IAM for ${OIDC_HOST}" >&2; exit 1; }

          ROLE_NAME="nat20-eks-ebs-csi-driver"
          if ! aws iam get-role --role-name "${ROLE_NAME}" >/dev/null 2>&1; then
            cat > /tmp/ebs-csi-trust.json <<JSON
          {
            "Version": "2012-10-17",
            "Statement": [
              {
                "Effect": "Allow",
                "Principal": { "Federated": "${PROVIDER_ARN}" },
                "Action": "sts:AssumeRoleWithWebIdentity",
                "Condition": {
                  "StringEquals": {
                    "${OIDC_HOST}:aud": "sts.amazonaws.com",
                    "${OIDC_HOST}:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa"
                  }
                }
              }
            ]
          }
          JSON
            aws iam create-role --role-name "${ROLE_NAME}" --assume-role-policy-document "file:///tmp/ebs-csi-trust.json" >/dev/null
            aws iam attach-role-policy --role-name "${ROLE_NAME}" --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
          fi
          ROLE_ARN="$(aws iam get-role --role-name "${ROLE_NAME}" --query 'Role.Arn' --output text)"

          # Create or update add-on
          if aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --region "${AWS_REGION}" --addon-name aws-ebs-csi-driver >/dev/null 2>&1; then
            aws eks update-addon --cluster-name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
              --addon-name aws-ebs-csi-driver --resolve-conflicts OVERWRITE --service-account-role-arn "${ROLE_ARN}" >/dev/null || true
          else
            aws eks create-addon --cluster-name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
              --addon-name aws-ebs-csi-driver --service-account-role-arn "${ROLE_ARN}" >/dev/null
          fi

          # Wait for ACTIVE
          for i in $(seq 1 60); do
            STATUS="$(aws eks describe-addon --cluster-name "${CLUSTER_NAME}" --region "${AWS_REGION}" --addon-name aws-ebs-csi-driver --query 'addon.status' --output text 2>/dev/null || true)"
            [ "${STATUS}" = "ACTIVE" ] && break
            sleep 5
          done
          [ "${STATUS}" = "ACTIVE" ] || { echo "EBS CSI add-on not ACTIVE (status=${STATUS:-unknown})" >&2; exit 1; }

          # Ensure gp3 StorageClass exists (idempotent)
          if ! kubectl get sc gp3 >/dev/null 2>&1; then
            cat <<'YAML' | kubectl apply -f -
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: gp3
          provisioner: ebs.csi.aws.com
          volumeBindingMode: WaitForFirstConsumer
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          parameters:
            type: gp3
            fsType: ext4
            encrypted: "true"
          YAML
          fi

      - name: Apply ClusterSecretStore and Mongo ExternalSecret
        shell: bash
        run: |
          set -euo pipefail
          kubectl apply -f infrastructure/k8s/secret-stores/clustersecretstore.yaml
          kubectl apply -n nat20 -f infrastructure/k8s/externalsecrets/mongo-secrets.yaml
          kubectl -n nat20 wait --for=condition=Ready externalsecret.external-secrets.io/mongo-root --timeout=300s
          for i in $(seq 1 60); do
            kubectl -n nat20 get secret mongo-root >/dev/null 2>&1 && break
            sleep 5
          done

      - name: Clean stale PVC if unbound (then deploy STS)
        shell: bash
        run: |
          set -euo pipefail
          if kubectl -n nat20 get pvc mongo-data-mongo-0 >/dev/null 2>&1; then
            PHASE="$(kubectl -n nat20 get pvc mongo-data-mongo-0 -o jsonpath='{.status.phase}' || true)"
            PVNAME="$(kubectl -n nat20 get pvc mongo-data-mongo-0 -o jsonpath='{.spec.volumeName}' || true)"
            if [ "${PHASE}" != "Bound" ] && [ -z "${PVNAME}" ]; then
              kubectl -n nat20 delete pvc mongo-data-mongo-0 --wait=true
            fi
          fi

          # Ensure Services first
          kubectl -n nat20 apply -f infrastructure/k8s/mongo/statefulset.yaml --server-side=false --force-conflicts=false

          # Wait for PVC to bind
          for i in $(seq 1 120); do
            PHASE="$(kubectl -n nat20 get pvc mongo-data-mongo-0 -o jsonpath='{.status.phase}' 2>/dev/null || true)"
            [ "${PHASE}" = "Bound" ] && break
            sleep 5
          done

          kubectl -n nat20 rollout status statefulset/mongo --timeout=900s

      - name: Output Mongo resources
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          kubectl get sc || true
          kubectl -n nat20 get pvc -o wide || true
          kubectl -n nat20 get pv -o wide || true
          kubectl -n nat20 get svc mongo -o wide || true
          kubectl -n nat20 get svc mongo-hl -o wide || true
          kubectl -n nat20 get sts mongo -o wide || true
          kubectl -n nat20 get pods -l app=mongo -o wide || true
          kubectl -n nat20 describe pvc mongo-data-mongo-0 || true
          kubectl -n nat20 get events --sort-by=.lastTimestamp | tail -n 200 || true

      - name: Restore EKS API CIDRs to original (idempotent)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          if [ -n "${ORIGINAL_EKS_API_CIDRS:-}" ]; then
            CURRENT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
              --query 'cluster.resourcesVpcConfig.publicAccessCidrs' --output text | sed 's/\t/,/g')"
            if [ "${CURRENT}" != "${ORIGINAL_EKS_API_CIDRS}" ]; then
              if ! aws eks update-cluster-config --name "${CLUSTER_NAME}" --region "${AWS_REGION}" \
                --resources-vpc-config publicAccessCidrs="${ORIGINAL_EKS_API_CIDRS}" 2>/tmp/eks_restore.err; then
                if ! grep -qi "already at the desired configuration" /tmp/eks_restore.err; then
                  cat /tmp/eks_restore.err >&2
                  exit 1
                fi
              fi
            fi
          fi
